{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33977041",
   "metadata": {},
   "source": [
    "# CHURN PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bc93e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "from scipy.stats import binom\n",
    "import perceptron\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee7b7f",
   "metadata": {},
   "source": [
    "## 1. Read input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ef53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Train Dataset\n",
    "df_start = pd.read_csv('data/p01_bank_data/bank_data_train.csv')\n",
    "\n",
    "# Read Final Test Dataset\n",
    "df_final = pd.read_csv('data/p01_bank_data/bank_data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe55a85",
   "metadata": {},
   "source": [
    "## 2. PreProcessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18dba8",
   "metadata": {},
   "source": [
    "### Add new calculated trend features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e01c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def add_trend_features(df):\n",
    "    tm1 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm1))\n",
    "    suffix_list = ['1M', '3M', 'TREND']\n",
    "    prefix_list = ['CNT_TRAN_ATM_TENDENCY',\n",
    "                   'CNT_TRAN_AUT_TENDENCY',\n",
    "                   'CNT_TRAN_CLO_TENDENCY',\n",
    "                   'CNT_TRAN_MED_TENDENCY',\n",
    "                   'CNT_TRAN_SUP_TENDENCY',\n",
    "                   'REST_DYNAMIC_CC_',\n",
    "                   'REST_DYNAMIC_CUR_',\n",
    "                   'REST_DYNAMIC_FDEP_',\n",
    "                   'REST_DYNAMIC_IL_',\n",
    "                   'REST_DYNAMIC_PAYM_',\n",
    "                   'SUM_TRAN_ATM_TENDENCY',\n",
    "                   'SUM_TRAN_AUT_TENDENCY',\n",
    "                   'SUM_TRAN_CLO_TENDENCY',\n",
    "                   'SUM_TRAN_MED_TENDENCY',\n",
    "                   'SUM_TRAN_SUP_TENDENCY',\n",
    "                   'TURNOVER_DYNAMIC_CC_',\n",
    "                   'TURNOVER_DYNAMIC_CUR_',\n",
    "                   'TURNOVER_DYNAMIC_IL_',\n",
    "                   'TURNOVER_DYNAMIC_PAYM_'\n",
    "                  ]\n",
    "    for item in prefix_list:\n",
    "        col0 = item + suffix_list[0]\n",
    "        col1 = item + suffix_list[1]\n",
    "        col2 = item + suffix_list[2]\n",
    "        df_wk = df[[col0, col1]]\n",
    "        df_wk['tmp0'] = df_wk[col0]/((df_wk[col1] - df_wk[col0])/2)\n",
    "        df.insert(1, col2, df_wk['tmp0'])\n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm1))\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be71357",
   "metadata": {},
   "source": [
    "### Replace NULL values by datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a8e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь для замены null по типам даннных\n",
    "dic_null = {}\n",
    "dic_null['int64'] = -1\n",
    "dic_null['float64'] = -1.0\n",
    "dic_null['object'] = 'N/A'\n",
    "\n",
    "# Формирование словаря со списками полей каждого типа данных\n",
    "def create_dic_type_list(df_in):\n",
    "    dic_start_types = {}\n",
    "    dataTypeSeries = df_in.dtypes\n",
    "    lst_ind = dataTypeSeries.index\n",
    "    lst_val = dataTypeSeries.values\n",
    "    for i in range(len(lst_ind)):\n",
    "        s = lst_val[i]\n",
    "        if s in dic_start_types.keys():\n",
    "            wk = dic_start_types[s]\n",
    "        else:\n",
    "            wk = []\n",
    "        wk.append(lst_ind[i])\n",
    "        dic_start_types[s] = wk\n",
    "    return dic_start_types\n",
    "\n",
    "# Замена пустых значений на значения из словаря\n",
    "def replace_null(df_in, dic_null, dic_start_types):\n",
    "    tm1 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm1))\n",
    "    for it in dic_start_types.keys():\n",
    "        val0 = dic_null['object']\n",
    "        if it == 'int64':\n",
    "            val0 = dic_null['int64']\n",
    "        if it == 'float64':\n",
    "            val0 = dic_null['float64']\n",
    "        df_in[dic_start_types[it]] = df_in[dic_start_types[it]].fillna(value=val0)\n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm1))    \n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b7d60",
   "metadata": {},
   "source": [
    "### Categorial features encoding by LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9712c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import string\n",
    "import bisect \n",
    "\n",
    "\n",
    "\n",
    "# Удаление множественных пробелов\n",
    "def remove_mult_spaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Удаление знаков пунктуации\n",
    "def remove_punctuation(text):\n",
    "    return re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "def replace_label_encoder(df, df1):\n",
    "    tm1 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm1))\n",
    "    categorical_features = list(df.select_dtypes(exclude=[np.number]).columns)\n",
    "    for col in categorical_features:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].apply(lambda text: remove_punctuation(text))\n",
    "        df[col] = df[col].apply(lambda text: remove_mult_spaces(text))\n",
    "        df1[col] = df1[col].str.lower()\n",
    "        df1[col] = df1[col].apply(lambda text: remove_punctuation(text))\n",
    "        df1[col] = df1[col].apply(lambda text: remove_mult_spaces(text))\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col])\n",
    "        \n",
    "        df1[col] = df1[col].map(lambda s: 'other' if s not in encoder.classes_ else s)\n",
    "        encoder.classes = encoder.classes_.tolist()\n",
    "        bisect.insort_left(encoder.classes, 'other')\n",
    "        encoder.classes_ = encoder.classes\n",
    "        df1[col] = encoder.transform(df1[col])\n",
    "        \n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm1))    \n",
    "    return df, df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e858477",
   "metadata": {},
   "source": [
    "### DataFrame MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b563cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def dataframe_scaler(df, df1):\n",
    "    tm1 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm1))\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = list(df.columns)\n",
    "    for i in range(1, len(cols) - 1):\n",
    "        col = cols[i]\n",
    "        arr = df[col].values  \n",
    "        data = arr.reshape(len(arr), 1)\n",
    "        scaler.fit(data) \n",
    "        lst = scaler.transform(data)\n",
    "        df[col] = pd.DataFrame(lst)\n",
    "        \n",
    "        arr1 = df1[col].values  \n",
    "        data1 = arr1.reshape(len(arr1), 1)\n",
    "        lst1 = scaler.transform(data1)\n",
    "        df1[col] = pd.DataFrame(lst1)\n",
    "\n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm1))    \n",
    "    return(df, df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff8301",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582567e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def feature_selection(df, df1):\n",
    "    tm0 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm0))\n",
    "    # Вычисление гиперпараметра альфа для лассо\n",
    "    tm1 = datetime.now()\n",
    "    print(\"   Start calc Alpha value \" + str(tm1))\n",
    "    use_columns = df.columns[1:-1]\n",
    "    target_columns = df.columns[df.shape[1] - 1:]\n",
    "    df_lasso = shuffle(df, random_state=42)\n",
    "    X = df_lasso[use_columns].to_numpy()\n",
    "    y = df_lasso[target_columns].to_numpy()\n",
    "    clf = LassoCV(cv=5, random_state=42)\n",
    "    clf.fit(X, y)\n",
    "    alpha = clf.alphas_[-1:][0]\n",
    "    print(\"   Time of calc Alpha value = \"+ str(datetime.now() - tm1))\n",
    "\n",
    "    # Модель лассо для выбора значимых фичей\n",
    "    tm1 = datetime.now()\n",
    "    print(\"   Start: Lasso \" + str(tm1))\n",
    "    lso = Lasso(alpha=alpha)\n",
    "    lso.fit(X, y)\n",
    "    print(\"   Time of Lasso = \"+ str(datetime.now() - tm1))\n",
    "\n",
    "    lst_select_features = []\n",
    "    lst_lasso = list(lso.coef_)\n",
    "    lst_feature_names = list(df_lasso.columns[1:-1])\n",
    "    for i in range(len(lst_feature_names)):\n",
    "        lst_select_features.append([lst_feature_names[i], lst_lasso[i]])\n",
    "    \n",
    "    lst_selected = [x for x in lst_select_features if abs(x[1]) > 0]\n",
    "    lst_selected = sorted(lst_selected, key=lambda x: x)\n",
    "\n",
    "    lst_drop = [x[0] for x in lst_select_features if abs(x[1]) == 0]\n",
    "    lst_drop = sorted(lst_drop, key=lambda x: x)\n",
    "    \n",
    "    df = df.drop(columns = lst_drop)\n",
    "    df1 = df1.drop(columns = lst_drop)\n",
    "\n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm0))    \n",
    "    return df, df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0617e",
   "metadata": {},
   "source": [
    "### Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5e43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from numpy import quantile, where, random\n",
    "\n",
    "def anomaly_detection(df):\n",
    "    tm1 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm1))\n",
    "    df_anomaly = df.iloc[:, 1 : -1].copy()\n",
    "    x = df_anomaly.to_numpy()\n",
    "    iforest = IsolationForest(n_estimators=1000)\n",
    "    iforest.fit(x)\n",
    "    scores = iforest.score_samples(x)\n",
    "    thresh = quantile(scores, 0.001)\n",
    "    index = where(scores <= thresh)\n",
    "    del_list = list(index[0])\n",
    "    df_anomaly_del = df.drop(del_list)\n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm1))    \n",
    "    return df_anomaly_del"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ddc6f",
   "metadata": {},
   "source": [
    "## 3. Execute Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d3fe67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start add_trend_features 2022-03-29 13:09:27.208466\n",
      "Time of add_trend_features = 0:00:06.515208\n",
      "Start add_trend_features 2022-03-29 13:09:34.512999\n",
      "Time of add_trend_features = 0:00:01.777491\n"
     ]
    }
   ],
   "source": [
    "df_trend = add_trend_features(df_start)\n",
    "df_trend_f = add_trend_features(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a47888c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start replace_null 2022-03-29 13:09:36.606571\n",
      "Time of replace_null = 0:00:01.013497\n",
      "Start replace_null 2022-03-29 13:09:37.620756\n",
      "Time of replace_null = 0:00:00.288369\n"
     ]
    }
   ],
   "source": [
    "dic_start_types = create_dic_type_list(df_trend)\n",
    "df_no_null = replace_null(df_trend, dic_null, dic_start_types)\n",
    "\n",
    "dic_start_types_f = create_dic_type_list(df_trend_f)\n",
    "df_no_null_f = replace_null(df_trend_f, dic_null, dic_start_types_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83183ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start replace_label_encoder 2022-03-29 13:09:37.918338\n",
      "Time of replace_label_encoder = 0:01:02.647138\n"
     ]
    }
   ],
   "source": [
    "df_label, df_label_f  = replace_label_encoder(df_no_null, df_no_null_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec528535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dataframe_scaler 2022-03-29 13:10:40.571450\n",
      "Time of dataframe_scaler = 0:00:00.835214\n"
     ]
    }
   ],
   "source": [
    "df_scaled, df_scaled_f = dataframe_scaler(df_label, df_label_f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12eb542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start feature_selection 2022-03-29 13:10:41.411420\n",
      "   Start calc Alpha value 2022-03-29 13:10:41.416907\n",
      "   Time of calc Alpha value = 0:01:13.350830\n",
      "   Start: Lasso 2022-03-29 13:11:54.767867\n",
      "   Time of Lasso = 0:00:56.005590\n",
      "Time of feature_selection = 0:02:09.652027\n"
     ]
    }
   ],
   "source": [
    "df_features_selection, df_features_selection_f = feature_selection(df_scaled, df_scaled_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d79723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start anomaly_detection 2022-03-29 13:17:17.470338\n",
      "Time of anomaly_detection = 0:07:33.789324\n"
     ]
    }
   ],
   "source": [
    "df_ready = anomaly_detection(df_features_selection)\n",
    "df_ready_f = df_features_selection_f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready.to_pickle('df_ready.pkl') \n",
    "df_ready = pd.read_pickle('data/df_ready.pkl')\n",
    "# df_ready_f.to_pickle('df_ready_f.pkl') \n",
    "df_ready_f = pd.read_pickle('data/df_ready_f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88c178eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 100)\n",
    "pd.set_option(\"max_columns\", 100)\n",
    "pd.set_option(\"max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a35f4",
   "metadata": {},
   "source": [
    "## 4. Stratified Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9480398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def create_train_test(df, not_neural=True, col_list=[]):\n",
    "    shuffle_index = []\n",
    "    if len(col_list) == 0:\n",
    "        use_columns = list(df.columns[1:-1])\n",
    "    else:\n",
    "        use_columns = col_list\n",
    "    target_column = list(df.columns[df.shape[1] - 1 : df.shape[1]])\n",
    "    X = df[use_columns].to_numpy()\n",
    "    if not_neural:\n",
    "        y = df[target_column].to_numpy()\n",
    "    else:\n",
    "        df = df.assign(TARGET_ADD = 1 - df.TARGET)\n",
    "        target_column = list(df.columns[df.shape[1] - 2 : df.shape[1]])\n",
    "        y = df[target_column].to_numpy()\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=0)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        shuffle_index.append([train_index, test_index])\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if not_neural:\n",
    "            y_train = y_train.reshape((y_train.shape[0],))\n",
    "            y_test = y_test.reshape((y_test.shape[0],))\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_final_input(df):\n",
    "    use_columns = list(df.columns[1:-1])\n",
    "    X = df[use_columns].to_numpy()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5379149f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ready' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9cf86b09eb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_Zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_final_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ready_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ready' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test(df_ready, True)\n",
    "X_Zero = create_final_input(df_ready_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83547c40",
   "metadata": {},
   "source": [
    "## 5. Funclions for models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497ce36",
   "metadata": {},
   "source": [
    "### Score functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bd6703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "def my_round(y_in):\n",
    "    y0 = []\n",
    "    ll = len(y_in)\n",
    "    for i in range(ll):\n",
    "        curr = y_in[i]\n",
    "        rr = round(curr)\n",
    "        y0.append(int(rr))\n",
    "    return y0\n",
    "\n",
    "def get_scores(y_true, y_pred):\n",
    "    roc = roc_auc_score(y_true, y_pred)\n",
    "    acc = accuracy_score(my_round(y_true), my_round(y_pred))\n",
    "    return roc #acc, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def GridSearchCV_best_param(model, param_grig, X_train, y_train):\n",
    "    tm0 = datetime.now()\n",
    "    f_name = inspect.getframeinfo(inspect.currentframe()).function\n",
    "    print(\"Start \" + f_name + \" \" + str(tm0))\n",
    "    grid_model = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n",
    "    grid_model.fit(X_train, y_train)\n",
    "    best_param = grid_model.best_params_\n",
    "    print(\"Time of \" + f_name + \" = \"+ str(datetime.now() - tm0))\n",
    "    return best_param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9537fe",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f26bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображение результатов моделирования\n",
    "list_rez = []\n",
    "\n",
    "def show_result_table(list_for_result=list_rez):\n",
    "    list_rez_columns = ['Library   ', 'Algorithms', 'Hyperparameters', 'Accuracy', 'AUC']\n",
    "    df_result = pd.DataFrame(list_for_result, columns=list_rez_columns)\n",
    "    df_result.style.set_properties(**{'text-align': 'left'})\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726c209",
   "metadata": {},
   "source": [
    "## 6. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db6f0a",
   "metadata": {},
   "source": [
    "### 6.1. Baseline: Naive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one = np.sum(y_train)\n",
    "train_zero = len(y_train) - train_one\n",
    "if train_one > train_zero:\n",
    "    naive_rez = 1.0\n",
    "else:\n",
    "    naive_rez = 0.0\n",
    "y_pred_naive = np.full((len(y_test), 1), naive_rez)\n",
    "y_pred_naive = y_pred_naive.reshape((y_pred_naive.shape[0],))\n",
    "acc, auc = get_scores(y_test, y_pred_naive)\n",
    "list_rez.append(['My_own_code', 'Naive classifier', 'No', acc, auc ])\n",
    "show_result_table() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2812f4",
   "metadata": {},
   "source": [
    "### 6.2. Baseline: RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e573caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=42,criterion='gini')\n",
    "param_grid = {\n",
    "    'n_jobs': [100],\n",
    "    'n_estimators': [300],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth' : [6]\n",
    "}\n",
    "best_param = GridSearchCV_best_param(rfc, param_grid, X_train[:20000], y_train[:20000])\n",
    "print(\"The best params for RandomForestClassifier:\")\n",
    "print(best_param)\n",
    "rfc_best = RandomForestClassifier(random_state=42,criterion='gini')\n",
    "rfc_best.set_params(**best_param)\n",
    "rfc_best.fit(X_train, y_train)\n",
    "y_pred_rfc = rfc_best.predict_proba(X_test)\n",
    "acc, auc = get_scores(y_test, y_pred_rfc[:, 1])\n",
    "list_rez.append(['sklearn.ensemble', 'RandomForestClassifier', best_param, acc, auc ])\n",
    "show_result_table() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b44fa0",
   "metadata": {},
   "source": [
    "### 6.3. MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_clf = MLPClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'solver': ['lbfgs'], \n",
    "    'max_iter': [100], \n",
    "    'alpha': [0.0001], \n",
    "    'hidden_layer_sizes': [100],\n",
    "    'activation': ['logistic' ]}\n",
    "\n",
    "best_param = GridSearchCV_best_param(MLP_clf, param_grid, X_train[:20000], y_train[:20000])\n",
    "print(\"The best params for MLPClassifier:\")\n",
    "print(best_param)\n",
    "MLP_best_clf = MLPClassifier(random_state=42)\n",
    "MLP_best_clf.set_params(**best_param)\n",
    "MLP_best_clf.fit(X_train, y_train)\n",
    "y_pred_mlp = MLP_best_clf.predict_proba(X_test)\n",
    "acc, auc = get_scores(y_test, y_pred_mlp[:, 1])\n",
    "list_rez.append(['sklearn.neural_network', 'MLPClassifier', best_param, acc, auc ])\n",
    "show_result_table() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f3749",
   "metadata": {},
   "source": [
    "### 6.4. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9003ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d6df2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test(df_ready, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e51e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_all = y_train.shape[0]\n",
    "# n_one = y_train.sum()\n",
    "# class_weight = {0: n_one/n_all, 1: (n_all - n_one)/n_all}\n",
    "# class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1ad8abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5442965888827105, 1: 6.143775430698641}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weight_arr = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    np.unique(y_train),\n",
    "    y_train)\n",
    "class_weight = {}\n",
    "for i in range(len(class_weight_arr)):\n",
    "    class_weight[i] = class_weight_arr[i]\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78e7a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = X_train.shape[1]\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "reduceLROnPlateau = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.00001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "early_stopping=EarlyStopping(monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             patience=20,\n",
    "                             restore_best_weights=True,\n",
    "                             min_delta=0.00001\n",
    "                            )\n",
    "keras_clf = keras.Sequential([\n",
    "  layers.Dense(50, \n",
    "               activation='relu', use_bias=False,\n",
    "#                kernel_regularizer = keras.regularizers.l2(l=0.0001),\n",
    "               input_shape=(input_nodes,)), \n",
    "#   layers.BatchNormalization(),\n",
    "    \n",
    "    \n",
    "  layers.Dense(20,\n",
    "#                kernel_regularizer = keras.regularizers.l2(l=0.0001),\n",
    "               activation='relu', use_bias=True),\n",
    "#   layers.BatchNormalization(),\n",
    "  layers.Dropout(0.25),\n",
    "    \n",
    "  layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "\n",
    "# keras_clf.compile(\n",
    "#     optimizer=keras.optimizers.SGD(\n",
    "#     learning_rate=learning_rate, momentum=0.9, nesterov=True\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "\n",
    "keras_clf.compile(\n",
    "    optimizer=tf.optimizers.Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07        \n",
    "        \n",
    "\n",
    "# keras_clf.compile(\n",
    "#     optimizer=keras.optimizers.Adam(\n",
    "#                                 beta_1=0.9,\n",
    "#                                 beta_2=0.999,\n",
    "#                                 learning_rate=learning_rate,\n",
    "#                                 epsilon=1e-08\n",
    "        \n",
    "                                    ),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = [keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b380b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 50)                4700      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,741\n",
      "Trainable params: 5,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c298e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_000 = y_train.reshape(-1)\n",
    "y_test_000 = y_test.reshape(-1)\n",
    "X_train_000 = X_train.copy()\n",
    "\n",
    "\n",
    "best_auc = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49bf6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.floatx()\n",
    "# tf.keras.backend.set_floatx('float64')\n",
    "# tf.keras.backend.floatx()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b80e1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_file = 'weights_keros.txt'\n",
    "# keras_clf.load_weights(w_file)\n",
    "# learning_rate = 6.88982775356993e-06\n",
    "# K.set_value(keras_clf.optimizer.learning_rate, learning_rate)\n",
    "# K.set_floatx('float64')\n",
    "#best_auc = 0.8080009905157872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37253069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "509a5f9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.6227 - auc: 0.7087 - val_loss: 0.6360 - val_auc: 0.7376 - lr: 0.0050\n",
      "0 0.005 0.7454044823327981 0.7399606437206401\n",
      "2555/2555 [==============================] - 3s 1ms/step - loss: 0.5961 - auc: 0.7427 - val_loss: 0.6767 - val_auc: 0.7554 - lr: 0.0050\n",
      "1 0.005 0.7595609446299565 0.7536639380059897\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5843 - auc: 0.7551 - val_loss: 0.6570 - val_auc: 0.7669 - lr: 0.0050\n",
      "2 0.005 0.7715870690650828 0.7690122944613083\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5783 - auc: 0.7625 - val_loss: 0.7056 - val_auc: 0.7594 - lr: 0.0050\n",
      "3 0.005 0.7680740874267769 0.7651606887534524\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5737 - auc: 0.7674 - val_loss: 0.6016 - val_auc: 0.7709 - lr: 0.0050\n",
      "4 0.005 0.7769441264899176 0.7702672300966378\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5717 - auc: 0.7690 - val_loss: 0.6395 - val_auc: 0.7737 - lr: 0.0050\n",
      "5 0.005 0.7762762652544021 0.7677485324818599\n",
      "2555/2555 [==============================] - 3s 1ms/step - loss: 0.5696 - auc: 0.7707 - val_loss: 0.5115 - val_auc: 0.7867 - lr: 0.0050\n",
      "6 0.005 0.7849165221719331 0.7774529537666666\n",
      "2555/2555 [==============================] - 3s 1ms/step - loss: 0.5670 - auc: 0.7737 - val_loss: 0.6513 - val_auc: 0.7779 - lr: 0.0050\n",
      "7 0.005 0.7823158022212313 0.7755852783708097\n",
      "2555/2555 [==============================] - 4s 2ms/step - loss: 0.5638 - auc: 0.7773 - val_loss: 0.5663 - val_auc: 0.7815 - lr: 0.0050\n",
      "8 0.005 0.7848548021211067 0.7784869612416547\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5640 - auc: 0.7776 - val_loss: 0.5192 - val_auc: 0.7825 - lr: 0.0050\n",
      "9 0.005 0.7882625467597276 0.7812065003449697\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5641 - auc: 0.7784 - val_loss: 0.5515 - val_auc: 0.7940 - lr: 0.0050\n",
      "10 0.005 0.7864295000022601 0.7759546808566058\n",
      "2555/2555 [==============================] - 3s 1ms/step - loss: 0.5620 - auc: 0.7795 - val_loss: 0.4892 - val_auc: 0.7989 - lr: 0.0050\n",
      "11 0.005 0.7946285674547849 0.7854120986208393\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5603 - auc: 0.7817 - val_loss: 0.6519 - val_auc: 0.7968 - lr: 0.0050\n",
      "12 0.005 0.7959366341054979 0.7861874545098695\n",
      "2555/2555 [==============================] - 6s 3ms/step - loss: 0.5565 - auc: 0.7837 - val_loss: 0.5204 - val_auc: 0.7813 - lr: 0.0050\n",
      "13 0.005 0.7880043044537036 0.7768755485957287\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5555 - auc: 0.7848 - val_loss: 0.5969 - val_auc: 0.7961 - lr: 0.0050\n",
      "14 0.005 0.7903398940388868 0.7825984202567602\n",
      "2555/2555 [==============================] - 3s 1ms/step - loss: 0.5545 - auc: 0.7853 - val_loss: 0.5606 - val_auc: 0.7994 - lr: 0.0050\n",
      "15 0.005 0.7993596799636415 0.7878810489577377\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5532 - auc: 0.7865 - val_loss: 0.7190 - val_auc: 0.7983 - lr: 0.0050\n",
      "16 0.005 0.7957382706078393 0.7874113591784907\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5500 - auc: 0.7893 - val_loss: 0.5824 - val_auc: 0.7996 - lr: 0.0050\n",
      "17 0.005 0.8001003034758333 0.7895050365214918\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5505 - auc: 0.7886 - val_loss: 0.5343 - val_auc: 0.8106 - lr: 0.0050\n",
      "18 0.005 0.8025891643780877 0.7927456499442018\n",
      "2555/2555 [==============================] - 7s 3ms/step - loss: 0.5476 - auc: 0.7916 - val_loss: 0.6236 - val_auc: 0.8025 - lr: 0.0050\n",
      "19 0.005 0.8038678142257338 0.7917096055184273\n",
      "2555/2555 [==============================] - 4s 2ms/step - loss: 0.5460 - auc: 0.7926 - val_loss: 0.5178 - val_auc: 0.8047 - lr: 0.0050\n",
      "20 0.005 0.8032359305890303 0.7927711012060831\n",
      "2555/2555 [==============================] - 6s 2ms/step - loss: 0.5438 - auc: 0.7935 - val_loss: 0.5043 - val_auc: 0.7959 - lr: 0.0050\n",
      "21 0.005 0.8049536510286757 0.7927116697808201\n",
      "2555/2555 [==============================] - 4s 1ms/step - loss: 0.5430 - auc: 0.7949 - val_loss: 0.4792 - val_auc: 0.8054 - lr: 0.0050\n",
      "22 0.005 0.8038776656696872 0.7915822934385306\n",
      "2555/2555 [==============================] - 5s 2ms/step - loss: 0.5421 - auc: 0.7967 - val_loss: 0.5948 - val_auc: 0.8016 - lr: 0.0050\n",
      "23 0.005 0.8046223060684361 0.7932460944934405\n",
      "2555/2555 [==============================] - 10s 4ms/step - loss: 0.5403 - auc: 0.7974 - val_loss: 0.6089 - val_auc: 0.7985 - lr: 0.0050\n",
      "24 0.005 0.8062937586513559 0.7934895924403005\n",
      "2555/2555 [==============================] - 11s 4ms/step - loss: 0.5395 - auc: 0.7977 - val_loss: 0.5988 - val_auc: 0.8047 - lr: 0.0050\n",
      "25 0.005 0.8091535285681242 0.7971180617419489\n",
      "2555/2555 [==============================] - 7s 3ms/step - loss: 0.5368 - auc: 0.7999 - val_loss: 0.5780 - val_auc: 0.8075 - lr: 0.0050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-d68fafe1fc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                             \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                            )\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0my_pred_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0my_pred_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mauc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_keras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1988\u001b[0m                                            batch_outputs)\n\u001b[1;32m   1989\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m               tf.__internal__.nest.map_structure_up_to(\n\u001b[0m\u001b[1;32m   1991\u001b[0m                   \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m                   \u001b[0;32mlambda\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m   \"\"\"\n\u001b[0;32m-> 1425\u001b[0;31m   return map_structure_with_tuple_paths_up_to(\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Discards the path arg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure_with_tuple_paths_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m       \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_path_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_value_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m   ]\n\u001b[0;32m-> 1528\u001b[0;31m   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\n\u001b[0m\u001b[1;32m   1529\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m   \"\"\"\n\u001b[0;32m--> 803\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             truncate(flat_sequence, 100), type(flat_sequence)))\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_nested_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "for i in range(10000):\n",
    "    shuffled_indices = np.random.permutation(len(y_train))\n",
    "    X_train = X_train[shuffled_indices]\n",
    "    y_train = y_train[shuffled_indices]\n",
    "    history = keras_clf.fit(X_train, \n",
    "                            y_train,\n",
    "                            class_weight=class_weight,\n",
    "                            batch_size=100,\n",
    "                            epochs=1,\n",
    "                            validation_split=0.1,\n",
    "                            callbacks=[early_stopping,\n",
    "                                       reduceLROnPlateau\n",
    "                                      ]\n",
    "                            ,use_multiprocessing=True\n",
    "                           )\n",
    "    y_pred_keras = keras_clf.predict(X_train_000)\n",
    "    y_pred_keras = y_pred_keras.reshape(-1)\n",
    "    auc1 = roc_auc_score(y_train_000, y_pred_keras)\n",
    "    \n",
    "    y_pred_keras = keras_clf.predict(X_test)\n",
    "    y_pred_keras = y_pred_keras.reshape(-1)\n",
    "    auc2 = roc_auc_score(y_test_000, y_pred_keras)\n",
    "    if auc2 > best_auc:\n",
    "        best_auc = auc2\n",
    "        with open('log_keros.txt', 'a') as the_file:\n",
    "            the_file.write(f'i = {i}, learning_rate = {learning_rate}, auc1 = {auc1}, auc2 = {auc2} \\n')\n",
    "            w_file = 'weights_keros.txt'\n",
    "            keras_clf.save_weights(w_file, overwrite=True)\n",
    "    print (i, learning_rate, auc1, auc2)\n",
    "    n_epochs -= 1\n",
    "    if n_epochs <= 0:\n",
    "        n_epochs = 10\n",
    "        learning_rate = learning_rate + 5e-5\n",
    "        w_file = 'weights_keros8277.txt'\n",
    "        # keras_clf.load_weights(w_file)\n",
    "        if learning_rate > 5e-3:\n",
    "            break\n",
    "        K.set_value(keras_clf.optimizer.learning_rate, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ff204",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_clf.save_weights(w_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29122bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343424e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = keras_clf.predict(X_train)\n",
    "y_train = y_train.reshape(-1)\n",
    "y_pred_keras = y_pred_keras.reshape(-1)\n",
    "auc = roc_auc_score(y_train, y_pred_keras)\n",
    "acc = accuracy_score(my_round(y_train), my_round(y_pred_keras))\n",
    "auc, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = keras_clf.predict(X_test)\n",
    "y_test = y_test.reshape(-1)\n",
    "y_pred_keras = y_pred_keras.reshape(-1)\n",
    "auc = roc_auc_score(y_test, y_pred_keras)\n",
    "acc = accuracy_score(my_round(y_test), my_round(y_pred_keras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "learning_rate = learning_rate * 0.9\n",
    "learning_rate = 5e-5\n",
    "K.set_value(keras_clf.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "w_file = 'weights_keros8277.txt'\n",
    "keras_clf.load_weights(w_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940ae54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a9b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9367c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Zero = keras_clf.predict(X_Zero)\n",
    "y_Zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = keras_clf.predict(X_test)\n",
    "y_test = y_test.reshape(-1)\n",
    "y_pred_keras = y_pred_keras.reshape(-1)\n",
    "y_pred = []\n",
    "for i in range(len(y_pred_keras)):\n",
    "    r = 0\n",
    "    if y_pred_keras[i]>0.5:\n",
    "        r=1\n",
    "    y_pred.append(r)\n",
    "sum(y_pred), sum(y_test)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_keras)\n",
    "acc = accuracy_score(my_round(y_test), my_round(y_pred_keras))\n",
    "\n",
    "auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'optimizer': 'Adam()',\n",
    "    'loss': 'BinaryCrossentropy(from_logits=True)',\n",
    "    'learning_rate': '1.8739346279424403e-05'\n",
    "}\n",
    "list_rez.append(['keras.layers', 'KERAS', params, acc, auc ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result_table(list_rez) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rez= [['My_own_code', 'Naive classifier', 'No', 0.918638240308876, 0.5],\n",
    " ['sklearn.ensemble',\n",
    "  'RandomForestClassifier',\n",
    "  {'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 300, 'n_jobs': 100},\n",
    "  0.918638240308876,\n",
    "  0.7966546545329787],\n",
    " ['sklearn.neural_network',\n",
    "  'MLPClassifier',\n",
    "  {'activation': 'logistic',\n",
    "   'alpha': 0.0001,\n",
    "   'hidden_layer_sizes': 100,\n",
    "   'max_iter': 100,\n",
    "   'solver': 'lbfgs'},\n",
    "  0.9189341524934125,\n",
    "  0.7246480914724593],\n",
    " ['keras.layers',\n",
    "  'KERAS',\n",
    "  {'optimizer': 'Adam()',\n",
    "   'loss': 'BinaryCrossentropy(from_logits=True)',\n",
    "   'learning_rate': '1.8739346279424403e-05'},\n",
    "  0.7111051615539618,\n",
    "  0.8245074490998133],\n",
    " ['tensorflow',\n",
    "  'tf.estimator.LinearClassifier',\n",
    "  {'num_epochs': '10', 'n_batch': '128', 'steps': '10000'},\n",
    "  0.9185819,\n",
    "  0.7152796]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open(\"list_rez.pkl\", \"wb\") as fp:\n",
    "#     pickle.dump(list_rez, fp)\n",
    "    \n",
    "# with open(\"list_rez.pkl\", \"rb\") as fp:  \n",
    "#     list_rez = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481895ee",
   "metadata": {},
   "source": [
    "## 6.5 Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = list(df_ready.columns[1:-1])\n",
    "LABEL = df_ready.columns[df_ready.shape[1] - 1 : df_ready.shape[1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_fn77(data_set, num_epochs=None, n_batch = 128, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "       x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "       y = pd.Series(data_set[LABEL].values),\n",
    "       batch_size=n_batch,   \n",
    "       num_epochs=num_epochs,\n",
    "       shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def create_train_val_test(df):\n",
    "    use_columns = list(df.columns[1:-1])\n",
    "    target_column = list(df.columns[df.shape[1] - 1 : df.shape[1]])\n",
    "    X = df[use_columns].to_numpy()\n",
    "    y = df[target_column].to_numpy()\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=0)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        df0 = df.iloc[train_index]\n",
    "        X_0 = df0[use_columns].to_numpy()\n",
    "        y_0 = df0[target_column].to_numpy()\n",
    "        sss_val = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=0)\n",
    "        for train_index0, val_index in sss.split(X_0, y_0):\n",
    "            df_train = df.iloc[train_index0].copy()\n",
    "            df_test = df.iloc[test_index].copy()\n",
    "            df_val = df.iloc[val_index].copy()\n",
    "    return df_train, df_test, df_val\n",
    "\n",
    "df_train, df_test, df_val = create_train_val_test(df_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af85ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_features = [tf.feature_column.numeric_column(k) for k in FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d40c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.LinearClassifier(\n",
    "    n_classes = 2,\n",
    "    feature_columns=my_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00364e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train(input_fn=get_input_fn77(df_train, \n",
    "                                      num_epochs=5,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27c74f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(input_fn=get_input_fn77(df_val, \n",
    "                                      num_epochs=5,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df0ace",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_res = model.evaluate(input_fn=get_input_fn77(df_test, \n",
    "                                      num_epochs=5,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df89600",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res['accuracy'], test_res['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rez = [['My_own_code', 'Naive classifier', 'No', 0.918638240308876, 0.5],\n",
    " ['sklearn.ensemble',\n",
    "  'RandomForestClassifier',\n",
    "  {'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 300, 'n_jobs': 100},\n",
    "  0.918638240308876,\n",
    "  0.7966546545329787],\n",
    " ['sklearn.neural_network',\n",
    "  'MLPClassifier',\n",
    "  {'activation': 'logistic',\n",
    "   'alpha': 0.0001,\n",
    "   'hidden_layer_sizes': 100,\n",
    "   'max_iter': 100,\n",
    "   'solver': 'lbfgs'},\n",
    "  0.9189341524934125,\n",
    "  0.7246480914724593],\n",
    " ['keras.layers',\n",
    "  'KERAS',\n",
    "  {'optimizer': 'keras.optimizers.Adam()',\n",
    "   'loss': 'keras.losses.BinaryCrossentropy(from_logits=True)',\n",
    "   'learning_rate': '0.0001'},\n",
    "  0.7802781574534643,\n",
    "  0.7651976526323951],\n",
    " ['tensorflow',\n",
    "  'tf.estimator.LinearClassifier',\n",
    "  {'num_epochs': '10', 'n_batch': '128', 'steps': '10000'},\n",
    "  0.9185819,\n",
    "  0.7152796]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "      'num_epochs': '10',\n",
    "      'n_batch': '128',\n",
    "      'steps': '10000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rez.append(['tensorflow', 'tf.estimator.LinearClassifier', params, test_res['accuracy'], test_res['auc']])\n",
    "# show_result_table() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result_table(list_rez)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5f00b",
   "metadata": {},
   "source": [
    "## 6.6 MLP by Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50413690",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test(df_ready, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43cfa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = perceptron.Net(n_hidden_layers = 2, debug=True, n_neurons=50, lr=0.01, n_epochs=10)\n",
    "my_net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d4ad7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred = my_net.predict(X_test)\n",
    "auc = roc_auc_score(y_test, my_pred)\n",
    "acc = accuracy_score(y_test, my_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "257436ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.918610058196063, 0.5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c02718a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    260765\n",
       "1    23102 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bafa00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {  'n_hidden_layers': '2',\n",
    "            'n_neurons': '50',\n",
    "            'lr': '0.01',\n",
    "            'n_epochs': '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77f6bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_rez = []\n",
    "list_rez.append(['perceptron', 'my_net', params, acc, auc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a370e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Library</th>\n",
       "      <th>Algorithms</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perceptron</td>\n",
       "      <td>my_net</td>\n",
       "      <td>{'n_hidden_layers': '2', 'n_neurons': '50', 'lr': '0.01', 'n_epochs': '2'}</td>\n",
       "      <td>0.91861</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Library    Algorithms  \\\n",
       "0  perceptron  my_net      \n",
       "\n",
       "                                                              Hyperparameters  \\\n",
       "0  {'n_hidden_layers': '2', 'n_neurons': '50', 'lr': '0.01', 'n_epochs': '2'}   \n",
       "\n",
       "   Accuracy  AUC  \n",
       "0  0.91861   0.5  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result_table(list_rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09164e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
